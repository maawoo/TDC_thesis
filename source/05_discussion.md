# Discussion

The following discussion is divided into three sections. Section @sec:usability-assessment discusses the results of the performed use cases and the usability of the TDC, which has been assessed thereby. Several possible improvements that are readily available and regard the integrated datasets are also presented. Section @sec:implementation-assessment focuses on technical aspects of the implementation that are related to ARD in general and the Open Data Cube. Finally, Section @sec:outlook provides an outlook on how the ARDCube tool could be extended, as well as the possible usage of the TDC at the Department of Earth Observation.  



## Usability Assessment

**Per-pixel computations**

By carrying out the use cases described in Section @sec:use-cases the usability of the initial implementation of the TDC could be assessed and possible improvements identified. The computation of per-pixel observations were a particular important aspect of the assessment. Even though the calculation of the sum of valid and clear-sky observations is rather simple, the underlying logistics of such a computation is not. This is especially the case if a very large volume of data is involved that, furthermore, is heterogeneous in nature (i.e., variability in spatial and temporal coverage). Various factors can have an influence on the performance and might only be apparent or lead to problems when a computation is performed at scale, as opposed to only using a small subset of the data. 

Fortunately, no significant issues were observed during the per-pixel computations of the TDC. In addition, some important aspects could be tested beforehand that are related to the parallel computing library Dask. No universal set of parameters exists that is always resulting in optimal performance. As the tests in Section @sec:performance-considerations have demonstrated, however, the available resources provided by Dask can easily be used to diagnose problems and adjust parameters accordingly. The visualization of diagnostics and insights as an interactive dashboard is reducing abstractness and the sense of working with a black box.     

The results of computing valid observations per pixel for the entire extent of the TDC (Figures @fig:pp_obs_s1_desc, @fig:pp_obs_s2, @fig:appendixfig_B1 and @fig:appendixfig_B2) provide valuable information about the temporal and spatial characteristics of the datasets. On one hand, inappropriate parameterization during the processing of ARD can be identified, such as a too large buffer for cloud, cloud shadow and snow detection. Optimal parameters should of course be determined beforehand by first processing a smaller subset of the dataset. However, a visualization of the end product can reveal inconsistencies in the underlying data that might otherwise remain unnoticed. Furthermore, it can be used as an additional source of information when performing time-series analysis. An area of interest can, for example, be located in a region of significantly fewer observations due to the orbit paths of the EO satellites. In addition, valid data points might be removed unintentionally when applying masks derived from the QAI band, as the algorithm used during processing can include false-positive detections [@Frantz2015; @Frantz2018]. It is important to consider these aspects before conducting an analysis or at least being aware of possible impacts. Having access to this information prior to the analysis or being able to easily generate and visualize it for a particular study area can be very beneficial. 

**Roda forest analysis**

The study area of the Roda forest is located in a region of fewer observations for the Sentinel-2 and Sentinel-1 descending datasets. Nevertheless, the amount of data points were sufficient to conduct the time-series analysis in the form that was intended and the potential of using the TDC could successfully be demonstrated.

As already described in Section @sec:roda_results, the highlighted forest area in Figure @fig:roda_analysis_1 contains patches that seem to have degraded over the observed timespan. Some of these patches are accompanied by increased VH backscatter values to the northeast, which correlates with the look direction of the ascending orbit pass of the Sentinel-1 satellites. This effect could be caused by the clearing of forest areas, as the backscatter increase might be due to a double-bounce mechanism along the newly created forest edge [@Villard2007]. An opposite shadow effect can sometimes be observed as well, which has been utilized by @Bouvet2018 to detect deforestation. However, a shadow effect does not seem to be visible in this case. The significant decrease of NDVI values for point P1 (Figure @fig:roda_analysis_2) and continuously low values thereafter, further support the assumption that at least some of these patches have not simply degraded but rather been cleared. The timing of the sharp drop-off of the NDVI time-series at the beginning of May 2018 suggests that the clearing happened at the beginning of the 2018 drought [@Schuldt2020]. A reason for such a clearing could be to prevent the spread of a bark beetle infestation. Damage caused by insect infestations has developed into the main reason for logging in German forests in recent years [@Destatis2021], hence increasing the need for monitoring solutions that use time-series information derived from EO data [e.g., @Hollaus2019; @FernandezCarrillo2020].

The lack of seasonality in the NDVI time-series for point P2 (Figure @fig:roda_analysis_2) might indicate that a coniferous evergreen forest is present in the area [@She2015]. A seasonal variation of the VH backscatter time-series on the other hand is plausible as the signal is influenced by the water content of the foliage, which changes over the year according to water availability [@Dubois2020]. The ascending and descending signals appear to show a more pronounced division during the summer periods 2018 and 2019 in comparison to 2017. As the orbits pass over the area during different times of the day (late afternoon for ascending; early morning for descending orbit), the diurnal difference of the backscatter signals could indicate drought related water stress [@SteeleDunne2012]. This effect needs to be investigated in more detail, however, to confirm or disprove this hypothesis.

Possible legacy effects of the 2018 drought can be observed in Figure @fig:appendixfig_B3. Most of the forest cover in the study region (see Figure @fig:roda_aoi) shows slightly lower median NDVI values in 2019 in comparison to 2018 when the drought occurred. This apparent decrease of vegetation health could be due to direct or indirect lagged impacts following the year of an extreme drought [@Frank2015]. As with the previous hypothesis, further investigation is needed to also exclude possible flaws in the methodology used for this analysis.

**Dataset improvements**

Data points from the Sentinel-2 and Landsat 8 datasets have been combined to calculate the median NDVI differences seen in Figures @fig:roda_analysis_1 and @fig:appendixfig_B3. This combined usage is not trivial, as both sensors cover slightly different spectral wavelengths. While effort is being made to create harmonized data products from both sensors [@Claverie2018; @Scheffler2020], ARD products that have been created with FORCE are not yet harmonized in terms of spectral wavelengths during processing. Therefore, this aspect needs to be considered when similar time-series analysis are conducted using the TDC and the appropriateness of aggregating data points from different datasets ultimately depends on the study objective.     

Currently, the Level-1C data products of the Sentinel-2 mission have a multi-temporal geometric uncertainty of around 12 m [@Gascon2017], which can have a negative influence on time-series analyses. While a geometric refinement is currently being implemented by ESA to improve the accuracy [@ESA2021], it is not clear if the reprocessing of past datasets is planned. A coregistration option has already been implemented into FORCE L2PS (see Figure @fig:force). @Rufin2020 describe the algorithm used in more detail, which ultimately leverages base images created from the Landsat 8 near-infrared band to improve the multi-temporal geometric uncertainty of the processed Sentinel-2 ARD product to an average of around 4.4 m. This processing option requires some preparation steps but is readily available in the version of FORCE utilized in the ARDCube tool. The quality of time-series can therefore be improved for future analyses performed with the TDC by reprocessing the Sentinel-2 dataset and performing the coregistration step.  

The Sentinel-1 datasets were already provided in an ARD format and have been processed using an SRTM 1 arcsecond DEM (see Section @sec:sar-satellite-data). As @Truckenbrodt2019 describe in their work, large discrepancies can sometimes be observed between various openly available DEM options, such as SRTM. This can affect the quality of topographic normalization during processing and ultimately the time-series analysis of individual pixels. Similarly to the Sentinel-2 dataset, it might be worthwhile to reprocess the dataset to further improve the data quality in regard to time-series analysis. The LiDAR-derived DEM utilized during the processing of the optical datasets could be used after a similar quality assessment as described by @Truckenbrodt2019 has been performed.  

**Concluding remarks**

In conclusion, the usability of the current implementation of the TDC has successfully been demonstrated with the performed use cases. The quality of all datasets is appropriate for time-series analysis but could be further improved with readily available processing options and ancillary data. Access to the indexed ARD products via the ODC Python API works without any issues or additional preparation steps. For example, no reprojection and resampling of the data has to be performed during loading of the data as Xarray datasets, as the data is already stored in a common CRS and the same non-overlapping tiling scheme. The utilization of a continental projection, such as GLANCE7, can be additionally advantageous in the future by facilitating interoperability with other study areas in the same region, provided that the same projection is used.   

The existing ecosystem of Python packages surrounding the core packages Xarray and Dask is steadily growing and has been adopted by a variety of scientific fields and institutions [e.g., @EynardBontemps2019]. The aspect of adoption should not be neglected in connection with open-source software projects, as it can ensure long term support of development. Various packages in this ecosystem can pave the way to more advanced types of analyses than demonstrated here. The Dask extension dask-ml [@DaskML-Software], for example, provides access to scalable machine learning by leveraging the popular Python library Scikit-Learn [@Pedregosa2011]. Furthermore, packages that might not be directly related to the ecosystem can easily be integrated into an analysis, such as the Roda use case. Additional information can thereby be provided, like climatic time-series data from weather stations located in the study area of interest [@Wetterdienst-Software].



## Implementation Assessment

### Analysis Ready Data {#sec:ard_discussion}

Processing optical and SAR data with the software components integrated in the ARDCube tool produces datasets that can directly be used in an analysis and hence be designated as ARD products. However, a formal assessment of how well these products comply with the current CARD4L specifications described in Section @sec:card4l, has neither been done in the course of this work, nor by the developers of FORCE or pyroSAR. However, @Truckenbrodt2019 acknowledge that this is a future goal along with a relevant extension of the pyroSAR software, and at the time of writing the official CARD4L website already lists FORCE as an ARD resource.

Various aspects that are related to ARD were taken into account while developing the ARDCube tool and implementing the TDC, but were not actively adjusted or changed. For the data format of both optical and SAR datasets, for example, the current setup solely relies upon which GeoTIFF specifications are defined by FORCE for the output files. @Alberti2018 demonstrates that GeoTIFF specifications, like the compression algorithm used, can have significant impacts on read and write speeds, as well as the ratio of compression and therefore storage size. Moreover, the internal tiling of the files affects the performance when only a small part of each file is accessed, which is done repeatedly when retrieving a pixel time-series, for example. These aspects need to be considered when large volumes of EO data are supposed to be handled during an analysis. Furthermore, new raster data formats have emerged in recent times, including COG and Zarr, which provide their own set of specifications, as well as advantages and disadvantages [@Yee2020]. In regard to the TDC, the current approach works quite well as demonstrated with the per-pixel computations (Section @sec:per-pixel-computations) and additionally, the option to use the higher-level processing system of FORCE [cf. @Frantz2019, pp. 9-15] is retained by not adjusting the output format after processing. Therefore, any changes of the data format need to be justified, as existing datasets and the processing workflows would need to be adjusted. There appears to be a lack of literature in regard to this topic, so an assessment where different raster data formats and their specification options are compared in the same computational setup could be valuable.

Similar to the data format, the handling of metadata needs further consideration. As mentioned in Sections @sec:optical-satellite-data and @sec:sar-satellite-data, additional metadata is stored in each GeoTIFF file by FORCE, whereas the SAR datasets were provided with separate metadata files. At present, the ARDCube tool does not include any ancillary functionality or leverages relevant features provided by pyroSAR, for example, to organize the available metadata. Another layer of complexity is added by the ODC, which requires its own set of metadata files in the form of dataset documents (see Section @sec:odc_methods), which currently only contain necessary information to ensure that the ODC Python API is operable. 

The CARD4L specifications list *machine readability* as one of the minimum requirements in terms of the handling of metadata, which is satisfied in all cases mentioned. Nevertheless, a uniform solution to organize and easily access the metadata of all datasets would be desirable. The SpatioTemporal Asset Catalog (STAC) specification [@STAC] could provide a viable and standardized solution, which potentially will be implemented into the ODC in the near future [@ODC-Docs3]. It uses a similar approach as used by the ODC in the form of higher-level (*Product Definition* in ODC; *Collections* in STAC) and lower-level metadata documents (*Dataset Documents* in ODC; *Items* in STAC). However, the STAC specification is an open-source project that is already being supported by a large community and currently receives a lot of attention in the geospatial domain. Openly available tools are actively being developed, such as the STAC Browser [@STACBrowser-Software], that provide intuitive methods of exploring the organized metadata.

A final aspect to be addressed is the availability of auxiliary data products, which are specified by CARD4L as *Per-Pixel Metadata* (see Section @sec:specifications). The QAI band produced by FORCE for the optical datasets is one such product already used in this implementation of the TDC. It provides valuable information during analyses and can be used to filter a time-series for clear-sky observations as demonstrated in Section @sec:per-pixel-computations. For the SAR datasets on the other hand, no auxiliary data products have currently been created and integrated into the TDC. The Normalized Radar Backscatter PFS of CARD4L lists, amongst others, the provision of a local incident angle image as a minimum requirement. In addition, @Truckenbrodt2019 recommend that a map of geometrical distortion (e.g., layover and radar shadow) should also be provided when a Sentinel-1 EODC is created. Both products can be produced during processing with pyroSAR's SNAP API and integrating them into the TDC could be of great value for future analyses and the development of new methodologies. 


### Open Data Cube {#sec:odc_discussion}

@Gentemann2021 state that a paradigm shift is happening in science, as data, software, and computational resources are moving towards cloud-based solutions. However, a lot of challenges are yet to be solved and HPC systems will remain important tools in many research institutions while technologies related to this shift are increasingly being adopted [@Abernathey2020]. In conjunction with this trend, various cloud-based EO platforms have emerged. As described by @Giuliani2019, such platforms potentially come with their own set of drawbacks and limit the control and flexibility of users. These aspects are particularly important in regard to research departments where existing computational resources are often utilized to perform data-intensive workloads and supplementary data sources might need to be integrated into analyses. Therefore, the ODC has been an appropriate choice for the initial implementation of the TDC.  

The complete setup of an operational EODC based on the ODC software library is still rather complex, as a lot of factors need to be considered and both IT and remote sensing knowledge is required. This problem has also been pointed out by @Giuliani2020b and @HernandezLopez2021 and is limiting the adoption of this technology. A possible solution has been proposed by @Giuliani2020b in the form of the Docker-based Data Cube on Demand (DCoD). This project intends to cover the entire cycle of downloading and processing EO data for a particular area of interest, as well as creating an ODC instance, with an automated and on-demand system. Being open-source with possible improvements and development of the proposed project through external contributors has been named by the authors as an additional advantage, but the DCoD has yet to be made public. 

The ARDCube project developed in the course of this work intends to solve the same problem as the DCoD, while focusing on the deployment on HPC systems. As mentioned in Section @sec:containerization, Singularity has been selected as the containerization solution for this reason, whereas Docker usually prevails as the most popular solution. This is also the case in regard to ODC, as various Docker-based ODC projects exist, but none have yet been found that are based on Singularity. Furthermore, the current documentation of ODC is lacking in clear guidance regarding the existing Docker-based projects, which is possibly resulting in confusion amongst ODC users and has complicated the search for an appropriate solution in regard to the TDC implementation.  

It has already been mentioned in Section @sec:ard_discussion that ODC is moving towards integrating the STAC specification. Similarly, the database backend is likely to change at some point in the future, as various proposals have been made to either replace the ODC database API [@Kouzoubov2019], which is currently relying on PostgreSQL, or to add support for alternative backends [@Woodcock2019; @Dhar2021]. However, it is yet unknown if and when these changes are implemented into the core library of ODC. 

A project that not only shares some of the same core Python packages as ODC but also drives their development forward, is Pangeo [@Pangeo-Web]. This community-driven project aims to enable big data geoscience research through an ecosystem of open-source software packages, such as the aforementioned Xarray, Dask and JupyterLab, and is already being used with large volumes of EO data as demonstrated by @Kellndorfer2021. As the ODC is still in a transformative state of development, it might be worthwhile to explore alternative solutions that can be used to organize and access the EO datasets of the TDC. The Pangeo software ecosystem is complemented by projects like Intake-STAC [@Hamman2020] and StackSTAC [@StackSTAC-Software], and might already facilitate the development of an architecture that is similar to what is being envisaged for the future of the ODC. However, a more in-depth comparison would be needed to identify advantages and disadvantages, as well as any technical limitations that can be present at the current stage of development.   

@Coetzee2020 have discussed the importance of communities that ultimately drive open-source projects forward in their development and that sustaining such a community can be challenging. This aspect is an important challenge for the future development of the ODC. Even though it is one of the pioneering projects regarding EODCs and has been deployed in various countries and regions worldwide, it still lacks an appropriate space for the community of users to engage sustainably. However, this challenge is likely being addressed, since the ODC has recently joined OSGeo (Open Source Geospatial Foundation) as a community project [@OSGeo-ODC] and is being supported via the Open Earth Alliance community activity of GEO (Group on Earth Observations) [@Gowda].



## Outlook

The ARDCube tool developed in the course of this work successfully facilitated the implementation of the TDC on the TerraSense HPC system. While the usage still requires a certain degree of IT and remote sensing knowledge, it can ease the process of creating an EODC for a region of interest and could potentially be used by other individuals or research departments. For this reason, the aim is to further develop the ARDCube tool and extend its functionality.

A particular area of improvement that might be of great potential is reproducibility. @Abernathey2020 have stated that the “reproducibility of data science projects requires open access to at least three elements: the code, the software environment, and the data.” (p. 3). They propose the concept of cloud-native data repositories, which could enable the reproducibility of scientific results that depend on large volumes of data and computational resources. However, various challenges remain that limit adoption of cloud computing in scientific research (e.g., funding). One of the few possible options for reproducibility in such cases is the recreation of the necessary software environment and datasets. Recreating a software environment and reprocessing data to the specific format necessary to reproduce scientific results comes with its own challenges, but can be eased by using containerization, for example. The ARDCube tool already provides the necessary foundation for reproducibility by managing multiple containerized software components and their parameterization. A possible improvement of the tool could be the automatic creation of some kind of *recipe* that defines all necessary information to recreate an EODC. This includes information such as the query used to download level-1 data, processing parameters for ARD generation, and software versions and dependencies to recreate the software environment. The recipe itself could be realized in the form of a simple machine-readable file (e.g., in the JSON format) or by utilizing an existing workflow management system like Snakemake [@Moelder2021]. 

Various improvements of the TDC are possible in the near future. Some improvements regarding the EO datasets that have already been implemented (Landsat 8, Sentinel-2A/B and Sentinel-1A/B) were suggested in the previous sections. The volume of data can be further extended by processing the data from other sensors of the Landsat archive via FORCE, which opens up the possibility to investigate time-series that span multiple decades. Other SAR datasets could be implemented as well via existing format drivers provided by pyroSAR. This would require some adjustments of the ARDCube tool, however, whereas processing of data from other Landsat sensors is immediately possible. Furthermore, the usage of Sentinel-1 Single Look Complex data in EODCs is being explored [@SAR2CUBE]. This would be a valuable addition to the TDC and enable the development of new methodologies for time-series analysis of SAR data.    

Once the volume of data has been extended temporally and in terms of additional sensors, the TDC could be kept up-to-date by automating the ARDCube workflows on TerraSense with a set of cron jobs [@cron-Software]. This could further be extended by automatically deriving information products from the TDC, which could be made accessible to the public through interactive web applications. The development of such an application could leverage existing projects of the ODC ecosystem [@Gowda2020; @ODC-Docs4].

Lastly, access to the TDC needs be improved to facilitate the efficient usage by multiple users and to avoid the possible misuse or accidental blocking of computational resources on TerraSense. This would especially be relevant in case working with the TDC is integrated into any existing lectures and a number of students need access simultaneously. Fortunately, solutions already exist that can also be adapted on TerraSense for the TDC, such as JupyterHub [@Thomas2021]. 

The integration of EODCs at the Remote Sensing Department of the University of Würzburg [@eo2cube] provides a possible outlook of how the TDC could be further utilized. Moreover, an increase in collaboration and sharing of knowledge between both departments could be of great potential. This might also include an effort to improve the interoperability between existing EODCs, which has been named by @Giuliani2019 as an essential challenge in order to prevent individual EODCs to become silos of information.  
